{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOfwE4Zn/FN6qqe84fjlE25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allaan08/Deep-Learning-Assignment-2/blob/main/assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5eKDQjgva_3",
        "outputId": "e0230cdf-94aa-40ed-f90c-760c6a05bb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Batch 100, Loss: 2.4208103263378145\n",
            "Epoch 1, Batch 200, Loss: 1.85990252494812\n",
            "Epoch 1, Batch 300, Loss: 1.6838390219211579\n",
            "Test Accuracy: 38.41%\n",
            "Epoch 2, Batch 100, Loss: 1.4759346628189087\n",
            "Epoch 2, Batch 200, Loss: 1.3979677259922028\n",
            "Epoch 2, Batch 300, Loss: 1.3245687448978425\n",
            "Test Accuracy: 51.8%\n",
            "Epoch 3, Batch 100, Loss: 1.1447507011890412\n",
            "Epoch 3, Batch 200, Loss: 1.07042323410511\n",
            "Epoch 3, Batch 300, Loss: 1.0384661781787872\n",
            "Test Accuracy: 61.85%\n",
            "Epoch 4, Batch 100, Loss: 0.904774232506752\n",
            "Epoch 4, Batch 200, Loss: 0.8900390112400055\n",
            "Epoch 4, Batch 300, Loss: 0.8785315251350403\n",
            "Test Accuracy: 66.83%\n",
            "Epoch 5, Batch 100, Loss: 0.7657712364196777\n",
            "Epoch 5, Batch 200, Loss: 0.7464985036849976\n",
            "Epoch 5, Batch 300, Loss: 0.7228716433048248\n",
            "Test Accuracy: 70.61%\n",
            "Epoch 6, Batch 100, Loss: 0.6691200521588325\n",
            "Epoch 6, Batch 200, Loss: 0.6354170340299606\n",
            "Epoch 6, Batch 300, Loss: 0.6159750410914421\n",
            "Test Accuracy: 75.87%\n",
            "Epoch 7, Batch 100, Loss: 0.5762758967280388\n",
            "Epoch 7, Batch 200, Loss: 0.5757929787039757\n",
            "Epoch 7, Batch 300, Loss: 0.5757783246040344\n",
            "Test Accuracy: 77.91%\n",
            "Epoch 8, Batch 100, Loss: 0.5305224570631981\n",
            "Epoch 8, Batch 200, Loss: 0.5353036406636238\n",
            "Epoch 8, Batch 300, Loss: 0.5462073835730553\n",
            "Test Accuracy: 76.47%\n",
            "Epoch 9, Batch 100, Loss: 0.5097106599807739\n",
            "Epoch 9, Batch 200, Loss: 0.5029440507292747\n",
            "Epoch 9, Batch 300, Loss: 0.5156805378198623\n",
            "Test Accuracy: 76.41%\n",
            "Epoch 10, Batch 100, Loss: 0.4906505784392357\n",
            "Epoch 10, Batch 200, Loss: 0.49820313811302186\n",
            "Epoch 10, Batch 300, Loss: 0.47266831815242766\n",
            "Test Accuracy: 81.45%\n",
            "Epoch 11, Batch 100, Loss: 0.4578596380352974\n",
            "Epoch 11, Batch 200, Loss: 0.47992042660713197\n",
            "Epoch 11, Batch 300, Loss: 0.4785037562251091\n",
            "Test Accuracy: 78.71%\n",
            "Epoch 12, Batch 100, Loss: 0.457977170497179\n",
            "Epoch 12, Batch 200, Loss: 0.4466144010424614\n",
            "Epoch 12, Batch 300, Loss: 0.4535552367568016\n",
            "Test Accuracy: 79.47%\n",
            "Epoch 13, Batch 100, Loss: 0.4362495931982994\n",
            "Epoch 13, Batch 200, Loss: 0.43725481882691386\n",
            "Epoch 13, Batch 300, Loss: 0.44525045424699783\n",
            "Test Accuracy: 81.26%\n",
            "Epoch 14, Batch 100, Loss: 0.4274435943365097\n",
            "Epoch 14, Batch 200, Loss: 0.42597476080060004\n",
            "Epoch 14, Batch 300, Loss: 0.4484415066242218\n",
            "Test Accuracy: 79.61%\n",
            "Epoch 15, Batch 100, Loss: 0.4021094563603401\n",
            "Epoch 15, Batch 200, Loss: 0.4313870725035667\n",
            "Epoch 15, Batch 300, Loss: 0.4154578921198845\n",
            "Test Accuracy: 80.16%\n",
            "Epoch 16, Batch 100, Loss: 0.4103327462077141\n",
            "Epoch 16, Batch 200, Loss: 0.410487704128027\n",
            "Epoch 16, Batch 300, Loss: 0.3882982891798019\n",
            "Test Accuracy: 79.31%\n",
            "Epoch 17, Batch 100, Loss: 0.39616637825965884\n",
            "Epoch 17, Batch 200, Loss: 0.40463315427303315\n",
            "Epoch 17, Batch 300, Loss: 0.41289590746164323\n",
            "Test Accuracy: 76.29%\n",
            "Epoch 18, Batch 100, Loss: 0.3879576924443245\n",
            "Epoch 18, Batch 200, Loss: 0.4072996300458908\n",
            "Epoch 18, Batch 300, Loss: 0.4003732654452324\n",
            "Test Accuracy: 73.85%\n",
            "Epoch 19, Batch 100, Loss: 0.38725803896784783\n",
            "Epoch 19, Batch 200, Loss: 0.39738688722252846\n",
            "Epoch 19, Batch 300, Loss: 0.3915118408203125\n",
            "Test Accuracy: 81.53%\n",
            "Epoch 20, Batch 100, Loss: 0.3766130003333092\n",
            "Epoch 20, Batch 200, Loss: 0.38334197267889975\n",
            "Epoch 20, Batch 300, Loss: 0.3732455672323704\n",
            "Test Accuracy: 81.08%\n",
            "Epoch 21, Batch 100, Loss: 0.3731721816956997\n",
            "Epoch 21, Batch 200, Loss: 0.37766431957483293\n",
            "Epoch 21, Batch 300, Loss: 0.38349638745188713\n",
            "Test Accuracy: 80.8%\n",
            "Epoch 22, Batch 100, Loss: 0.3634716975688934\n",
            "Epoch 22, Batch 200, Loss: 0.37383238583803174\n",
            "Epoch 22, Batch 300, Loss: 0.38569260001182554\n",
            "Test Accuracy: 78.85%\n",
            "Epoch 23, Batch 100, Loss: 0.3533728656172752\n",
            "Epoch 23, Batch 200, Loss: 0.37054158851504326\n",
            "Epoch 23, Batch 300, Loss: 0.36762855663895605\n",
            "Test Accuracy: 82.63%\n",
            "Epoch 24, Batch 100, Loss: 0.3568565298616886\n",
            "Epoch 24, Batch 200, Loss: 0.3690172049403191\n",
            "Epoch 24, Batch 300, Loss: 0.3781085057556629\n",
            "Test Accuracy: 82.07%\n",
            "Epoch 25, Batch 100, Loss: 0.3486485643684864\n",
            "Epoch 25, Batch 200, Loss: 0.34537539273500445\n",
            "Epoch 25, Batch 300, Loss: 0.3707599024474621\n",
            "Test Accuracy: 84.31%\n",
            "Epoch 26, Batch 100, Loss: 0.3566426096856594\n",
            "Epoch 26, Batch 200, Loss: 0.3569246906042099\n",
            "Epoch 26, Batch 300, Loss: 0.3501864831149578\n",
            "Test Accuracy: 81.81%\n",
            "Epoch 27, Batch 100, Loss: 0.3705593077838421\n",
            "Epoch 27, Batch 200, Loss: 0.3664243285357952\n",
            "Epoch 27, Batch 300, Loss: 0.3584138417243958\n",
            "Test Accuracy: 82.93%\n",
            "Epoch 28, Batch 100, Loss: 0.34733456283807757\n",
            "Epoch 28, Batch 200, Loss: 0.35558638751506805\n",
            "Epoch 28, Batch 300, Loss: 0.3599918645620346\n",
            "Test Accuracy: 82.74%\n",
            "Epoch 29, Batch 100, Loss: 0.34444489687681196\n",
            "Epoch 29, Batch 200, Loss: 0.3473063163459301\n",
            "Epoch 29, Batch 300, Loss: 0.36090554341673853\n",
            "Test Accuracy: 82.69%\n",
            "Epoch 30, Batch 100, Loss: 0.3341206005215645\n",
            "Epoch 30, Batch 200, Loss: 0.36737998709082603\n",
            "Epoch 30, Batch 300, Loss: 0.36565286308526995\n",
            "Test Accuracy: 83.0%\n",
            "Epoch 31, Batch 100, Loss: 0.25169732823967933\n",
            "Epoch 31, Batch 200, Loss: 0.19254330329596997\n",
            "Epoch 31, Batch 300, Loss: 0.1757089913636446\n",
            "Test Accuracy: 91.85%\n",
            "Epoch 32, Batch 100, Loss: 0.163919810205698\n",
            "Epoch 32, Batch 200, Loss: 0.15542586833238603\n",
            "Epoch 32, Batch 300, Loss: 0.1493511165678501\n",
            "Test Accuracy: 92.06%\n",
            "Epoch 33, Batch 100, Loss: 0.1266410969570279\n",
            "Epoch 33, Batch 200, Loss: 0.12780393697321416\n",
            "Epoch 33, Batch 300, Loss: 0.13328066673129796\n",
            "Test Accuracy: 92.69%\n",
            "Epoch 34, Batch 100, Loss: 0.11741595976054668\n",
            "Epoch 34, Batch 200, Loss: 0.12031921043992043\n",
            "Epoch 34, Batch 300, Loss: 0.11538079265505076\n",
            "Test Accuracy: 92.74%\n",
            "Epoch 35, Batch 100, Loss: 0.09936161132529378\n",
            "Epoch 35, Batch 200, Loss: 0.1047443930245936\n",
            "Epoch 35, Batch 300, Loss: 0.11011911086738109\n",
            "Test Accuracy: 92.74%\n",
            "Epoch 36, Batch 100, Loss: 0.09036786627024412\n",
            "Epoch 36, Batch 200, Loss: 0.09316930580884218\n",
            "Epoch 36, Batch 300, Loss: 0.09526825740933419\n",
            "Test Accuracy: 92.73%\n",
            "Epoch 37, Batch 100, Loss: 0.081008264683187\n",
            "Epoch 37, Batch 200, Loss: 0.08978037098422646\n",
            "Epoch 37, Batch 300, Loss: 0.08400390043854714\n",
            "Test Accuracy: 92.53%\n",
            "Epoch 38, Batch 100, Loss: 0.07872228097170592\n",
            "Epoch 38, Batch 200, Loss: 0.07585688913241029\n",
            "Epoch 38, Batch 300, Loss: 0.07872651690617204\n",
            "Test Accuracy: 92.83%\n",
            "Epoch 39, Batch 100, Loss: 0.06587999496608972\n",
            "Epoch 39, Batch 200, Loss: 0.07013557903468609\n",
            "Epoch 39, Batch 300, Loss: 0.07152811624109745\n",
            "Test Accuracy: 92.65%\n",
            "Epoch 40, Batch 100, Loss: 0.0622800899669528\n",
            "Epoch 40, Batch 200, Loss: 0.06206970121711493\n",
            "Epoch 40, Batch 300, Loss: 0.06993528874590993\n",
            "Test Accuracy: 92.34%\n",
            "Epoch 41, Batch 100, Loss: 0.06477484486997127\n",
            "Epoch 41, Batch 200, Loss: 0.05882886359468102\n",
            "Epoch 41, Batch 300, Loss: 0.06053701695986092\n",
            "Test Accuracy: 92.5%\n",
            "Epoch 42, Batch 100, Loss: 0.05209742346778512\n",
            "Epoch 42, Batch 200, Loss: 0.05748622380197048\n",
            "Epoch 42, Batch 300, Loss: 0.05586413033306599\n",
            "Test Accuracy: 92.57%\n",
            "Epoch 43, Batch 100, Loss: 0.05091344790533185\n",
            "Epoch 43, Batch 200, Loss: 0.047970018247142435\n",
            "Epoch 43, Batch 300, Loss: 0.06177153464406729\n",
            "Test Accuracy: 92.44%\n",
            "Epoch 44, Batch 100, Loss: 0.0537720401212573\n",
            "Epoch 44, Batch 200, Loss: 0.046172457989305256\n",
            "Epoch 44, Batch 300, Loss: 0.04920591534115374\n",
            "Test Accuracy: 92.3%\n",
            "Epoch 45, Batch 100, Loss: 0.045744822379201654\n",
            "Epoch 45, Batch 200, Loss: 0.04754407847300172\n",
            "Epoch 45, Batch 300, Loss: 0.05603217467665672\n",
            "Test Accuracy: 92.25%\n",
            "Epoch 46, Batch 100, Loss: 0.048276615533977746\n",
            "Epoch 46, Batch 200, Loss: 0.05374435951001942\n",
            "Epoch 46, Batch 300, Loss: 0.050749305430799724\n",
            "Test Accuracy: 92.31%\n",
            "Epoch 47, Batch 100, Loss: 0.044623449128121134\n",
            "Epoch 47, Batch 200, Loss: 0.049416881427168845\n",
            "Epoch 47, Batch 300, Loss: 0.05089680417440832\n",
            "Test Accuracy: 92.55%\n",
            "Epoch 48, Batch 100, Loss: 0.044468366783112286\n",
            "Epoch 48, Batch 200, Loss: 0.047009182861074805\n",
            "Epoch 48, Batch 300, Loss: 0.04660442031919956\n",
            "Test Accuracy: 92.04%\n",
            "Epoch 49, Batch 100, Loss: 0.04336821514181793\n",
            "Epoch 49, Batch 200, Loss: 0.04950720948167145\n",
            "Epoch 49, Batch 300, Loss: 0.043933109790086744\n",
            "Test Accuracy: 92.32%\n",
            "Epoch 50, Batch 100, Loss: 0.03793137365486473\n",
            "Epoch 50, Batch 200, Loss: 0.046099757822230455\n",
            "Epoch 50, Batch 300, Loss: 0.04837271172553301\n",
            "Test Accuracy: 91.92%\n",
            "\n",
            "Confusion Matrix:\n",
            " [[920   0  22   6   5   1   1   3  38   4]\n",
            " [  7 962   0   2   1   0   3   0  10  15]\n",
            " [ 23   0 896  36  12  18  10   3   2   0]\n",
            " [  8   1  38 849  10  64  17   8   5   0]\n",
            " [  1   0  22  28 910  27   5   6   1   0]\n",
            " [  5   0  11  75   4 894   3   7   1   0]\n",
            " [  6   0  25  22   2   3 939   3   0   0]\n",
            " [  5   0   6  21  12  25   0 929   2   0]\n",
            " [ 15   3   3   7   0   1   1   1 967   2]\n",
            " [ 14  27   1   5   1   2   4   2  18 926]]\n",
            "Precision: 0.92\n",
            "Recall: 0.92\n",
            "F1 Score: 0.92\n",
            "AU-ROC: N/A (single-class prediction issue)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Modify ResNet18 for CIFAR-10\n",
        "class ResNetCIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNetCIFAR, self).__init__()\n",
        "        self.model = resnet18(pretrained=False)\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.model.maxpool = nn.Identity()\n",
        "        self.model.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = ResNetCIFAR(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"Epoch {epoch}, Batch {batch_idx+1}, Loss: {running_loss/100}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Testing function with metrics\n",
        "def test(final_epoch=False):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_targets = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy}%\")\n",
        "\n",
        "    # Only display additional metrics at the final epoch\n",
        "    if final_epoch:\n",
        "        conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
        "        class_report = classification_report(all_targets, all_predictions, output_dict=True)\n",
        "        precision = class_report[\"weighted avg\"][\"precision\"]\n",
        "        recall = class_report[\"weighted avg\"][\"recall\"]\n",
        "        f1 = class_report[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "        # Convert to binary labels for AU-ROC computation for each class\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(all_targets, np.array(all_predictions).reshape(-1, 1), multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            roc_auc = \"N/A (single-class prediction issue)\"\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        print(f\"Recall: {recall:.2f}\")\n",
        "        print(f\"F1 Score: {f1:.2f}\")\n",
        "        print(f\"AU-ROC: {roc_auc}\")\n",
        "\n",
        "# Train and evaluate\n",
        "for epoch in range(1, 51):  # Run for 50 epochs\n",
        "    train(epoch)\n",
        "    if epoch == 50:\n",
        "        test(final_epoch=True)  # Final epoch with additional metrics\n",
        "    else:\n",
        "        test()\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "Jvd_sLfTJSLs",
        "outputId": "d617d729-cea9-425c-ba4f-6590d4e4670f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-1-2a16d7e3e942>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2a16d7e3e942>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git remote add origin https://github.com/allaan08/Deep-Learning-Assignment-2.git\u001b[0m\n\u001b[0m                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    }
  ]
}